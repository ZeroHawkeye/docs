---
title: "使用 Groq Llama 3 构建'网站聊天'功能"
description: "学习如何使用 Firecrawl、Groq Llama 3 和 Langchain 构建'与您的网站聊天'机器人。"
og:title: "使用 Groq Llama 3 构建'网站聊天'功能 | Firecrawl"
og:description: "学习如何使用 Firecrawl、Groq Llama 3 和 Langchain 构建'与您的网站聊天'机器人。"
---

> 注意：此示例使用的是 [Firecrawl API 的 v0 版本](/v0/introduction)。您可以安装 Python SDK 的 0.0.20 版本或 Node SDK 的 0.0.36 版本。


## 环境设置

安装我们的 Python 依赖项，包括 langchain、groq、faiss、ollama 和 firecrawl-py。

```bash
pip install --upgrade --quiet langchain langchain-community groq faiss-cpu ollama firecrawl-py
```

我们将使用 Ollama 进行嵌入，您可以在[这里](https://ollama.com/)下载 Ollama。当然，您也可以使用任何其他您喜欢的嵌入方式。

## 使用 Firecrawl 加载网站

为了能够获取网站的所有数据并确保其格式最为清晰，我们将使用 Firecrawl。Firecrawl 作为文档加载器可以非常容易地与 Langchain 集成。

以下是如何使用 Firecrawl 加载网站：

```python
from langchain_community.document_loaders import FireCrawlLoader  # 导入 FirecrawlLoader

url = "https://firecrawl.dev"
loader = FireCrawlLoader(
    api_key="fc-YOUR_API_KEY", # 注意：将 'YOUR_API_KEY' 替换为您实际的 FireCrawl API 密钥
    url=url,  # 要爬取的目标 URL
    mode="crawl"  # 模式设置为 'crawl' 以爬取所有可访问的子页面
)
docs = loader.load()
```

## 设置向量存储

接下来，我们将设置向量存储。向量存储是一种允许我们存储和查询嵌入的数据结构。我们将使用 Ollama 嵌入和 FAISS 向量存储。
我们将文档分割成每个 1000 个字符的块，每块之间有 200 个字符的重叠。这是为了确保块既不会太小也不会太大 - 并且在我们查询时可以适合 LLM 模型。

```python
from langchain_community.embeddings import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents=splits, embedding=OllamaEmbeddings())
```

## 检索和生成

现在我们的文档已加载并且向量存储已设置好，我们可以根据用户的问题进行相似性搜索，以检索最相关的文档。这样我们就可以使用这些文档来输入到 LLM 模型中。

```python
question = "What is firecrawl?"
docs = vectorstore.similarity_search(query=question)
```

## 生成

最后但同样重要的是，您可以使用 Groq 根据我们加载的文档生成对问题的回答。

```python
from groq import Groq

client = Groq(
    api_key="YOUR_GROQ_API_KEY",
)

completion = client.chat.completions.create(
    model="llama3-8b-8192",
    messages=[
        {
            "role": "user",
            "content": f"You are a friendly assistant. Your job is to answer the users question based on the documentation provided below:\nDocs:\n\n{docs}\n\nQuestion: {question}"
        }
    ],
    temperature=1,
    max_tokens=1024,
    top_p=1,
    stream=False,
    stop=None,
)

print(completion.choices[0].message)
```

## 大功告成！

您现在已经使用 Llama 3、Groq Llama 3、Langchain 和 Firecrawl 构建了一个"与您的网站聊天"的机器人。您现在可以使用这个机器人根据您网站的文档回答问题。

如果您有任何问题或需要帮助，请随时通过 [Firecrawl](https://firecrawl.dev) 联系我们。
